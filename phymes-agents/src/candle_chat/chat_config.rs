use anyhow::Result;
use clap::Parser;
use serde::{Deserialize, Serialize};

use crate::candle_assets::candle_which::WhichCandleAsset;
use crate::openai_asset::openai_which::WhichOpenAIAsset;

/// Configuration for chat completion
///
/// # Notes
///
/// Not 100% OpenAI API compatible
/// see <https://platform.openai.com/docs/api-reference/chat/create> for differences
#[derive(Parser, Debug, Serialize, Deserialize, Default, Clone)]
#[command(author, version, about, long_about = None)]
pub struct CandleChatConfig {
    /// GGUF file to load, typically a .gguf file generated by the quantize command from llama.cpp
    #[arg(long)]
    pub weights_file: Option<String>,

    /// The tokenizer config in json format.
    #[arg(long)]
    pub tokenizer_file: Option<String>,

    /// The model weights config in json format.
    #[arg(long)]
    pub weights_config_file: Option<String>,

    /// The tokenizer config config file in json format.
    #[arg(long)]
    pub tokenizer_config_file: Option<String>,

    /// The API endpoint for chat completion
    #[arg(long)]
    pub api_url: Option<String>,

    /// A list of messages comprising the conversation so far
    #[arg(long)]
    pub messages: Option<String>,

    /// The length of the sample to generate (in tokens).
    #[arg(short = 'n', long, default_value_t = 1000)]
    pub max_tokens: usize,

    /// The temperature used to generate samples, use 0 for greedy sampling.
    #[arg(long, default_value_t = 0.8)]
    pub temperature: f64,

    /// Nucleus sampling probability cutoff.
    #[arg(long)]
    pub top_p: Option<f64>,

    /// Only sample among the top K samples.
    #[arg(long)]
    pub top_k: Option<usize>,

    /// The seed to use when generating random samples.
    #[arg(long, default_value_t = 299792458)]
    pub seed: u64,

    /// Process prompt elements separately.
    #[arg(long)]
    pub split_prompt: bool,

    /// Run on CPU rather than GPU even if a GPU is available.
    #[arg(long)]
    pub cpu: bool,

    /// Penalty to be applied for repeating tokens, 1. means no penalty.
    #[arg(long, default_value_t = 1.1)]
    pub repeat_penalty: f32,

    /// The context size to consider for the repeat penalty.
    #[arg(long, default_value_t = 64)]
    pub repeat_last_n: usize,

    /// Number between -2.0 and 2.0.
    /// Positive values penalize new tokens based on their existing frequency in the text so far,
    /// decreasing the model's likelihood to repeat the same line verbatim
    #[arg(long, default_value_t = 0.0)]
    pub frequency_penalty: f32,

    /// The model to use.
    #[arg(long)]
    pub candle_asset: Option<WhichCandleAsset>,

    /// The model to use.
    #[arg(long)]
    pub openai_asset: Option<WhichOpenAIAsset>,
}

impl CandleChatConfig {
    pub fn new_from_json(input: &str) -> Result<Self> {
        let self_data: CandleChatConfig = serde_json::from_str(input)?;
        Ok(self_data)
    }
}
