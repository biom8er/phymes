use anyhow::Result;
use clap::{Parser, ValueEnum};
use serde::{Deserialize, Serialize};

use crate::candle_assets::candle_which::WhichCandleAsset;
use crate::openai_asset::openai_which::WhichOpenAIAsset;

#[derive(Debug, ValueEnum, PartialEq, Clone, Eq, Serialize, Deserialize)]
pub enum Pool {
    /// Select the CLS token as embedding
    Cls,
    /// Apply Mean pooling to the model embeddings
    Mean,
    /// Apply SPLADE (Sparse Lexical and Expansion) to the model embeddings.
    /// This option is only available if the loaded model is a `ForMaskedLM` Transformer
    /// model.
    Splade,
    /// Select the last token as embedding
    LastToken,
}

/// Config for text embedding inference (TEI)
///
/// see the NVIDIA NIMs reference <https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/reference.html>
/// see the OpenAI reference <https://platform.openai.com/docs/guides/embeddings>
#[derive(Parser, Debug, Serialize, Deserialize, Default, Clone)]
#[command(author, version, about, long_about = None)]
pub struct CandleEmbedConfig {
    /// GGUF file to load, typically a .gguf file generated by the quantize command from llama.cpp
    #[arg(long)]
    pub weights_file: Option<String>,

    /// list of queries formatted as a JSON string e.g., "['query1', 'query2']"
    #[arg(long)]
    pub embed_queries: Option<String>,

    /// The tokenizer config in json format.
    #[arg(long)]
    pub tokenizer_file: Option<String>,

    /// The model weights config in json format.
    #[arg(long)]
    pub weights_config_file: Option<String>,

    /// The tokenizer config config file in json format.
    #[arg(long)]
    pub tokenizer_config_file: Option<String>,

    /// The API endpoint for chat completion
    #[arg(long)]
    pub api_url: Option<String>,

    /// The number of dimensions the resulting output embeddings should have.
    #[arg(long)]
    pub dimensions: Option<i32>,

    /// The format to return the embeddings in. Can be either float or base64
    #[arg(long, default_value = "float")]
    pub encoding_format: String,

    /// The mode to use for embedding. Can be either passage or query
    #[arg(long, default_value = "query")]
    pub input_type: String,

    /// The modality to embed. Can be either be text, image, text_image
    #[arg(long, default_value = "text")]
    pub modality: String,

    /// Run on CPU rather than GPU even if a GPU is available.
    #[arg(long)]
    pub cpu: bool,

    /// The model size to use.
    #[arg(long, default_value = "Qwen-v2-1.5b-embed")]
    pub candle_asset: Option<WhichCandleAsset>,

    /// The model to use.
    #[arg(long)]
    pub openai_asset: Option<WhichOpenAIAsset>,
}

impl CandleEmbedConfig {
    #[allow(dead_code)]
    fn new_from_json(input: &&str) -> Result<Self> {
        let self_data: CandleEmbedConfig = serde_json::from_str(input)?;
        Ok(self_data)
    }
}
