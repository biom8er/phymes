use anyhow::Error;
use new_string_template::template::Template;
use std::collections::HashMap;
use tokenizers::{
    Tokenizer,
    utils::padding::{PaddingDirection, PaddingParams, PaddingStrategy},
};

/**
Returns a new Vec of prompt String

# Arguments

* `template` - string literal for the chat prompt
* `data` - Vec of queries (both queries and documents) to be embedded
* `system_prompt` - String with the instructions

*/
pub fn create_prompt_embed(
    template: &str,
    data: &[String],
    system_prompt: &String,
) -> anyhow::Result<Vec<String>> {
    let prompt = Template::new(template);

    let prompt_system = prompt.render_nofail(&HashMap::from([("instructions", system_prompt)]));
    let prompt_system = Template::new(prompt_system.as_str());

    let prompt_strs: Vec<String> = data
        .iter()
        .map(|row| prompt_system.render_nofail(&HashMap::from([("query", row)])))
        .collect();

    Ok(prompt_strs)
}

type ProcessPromptEmbedOutput = Vec<Vec<u32>>;
/**
Return the prompt tokens as Tensors optionally shortening to the maximum input length

# Arguments

* `prompts` - `Vec<String>` of the chat prompt generated by `create_prompt_embed`
* `tokenizer` - `Tokenizer` to use for generating the tokens
* `eos_token_id` - `u32` id of the end of sentence token used for padding
* `eos_token` - `&str` literal of the end of sentence token used for padding
* `max_seq_length` - `Optional<usize>` of the maximum input length. Note that this feature is currently not implemented
* `device` - `Device` from Candle used to create the Tensors

# Returns

* `tokens` - `Tensor` of tokens of dimension prompt_length by # of prompts
  Note that the tokens are currently returned directly due to memory borrowing issues
  during the token generation process using `Tokenizer` for batch embedding
* `masks` - `Tensor` of tokens of dimension prompt_length by # of prompts

*/
pub fn process_prompt_embed(
    prompts: &[String],
    tokenizer: &mut Tokenizer,
    eos_token_id: u32,
    eos_token: &str,
    _max_seq_length: Option<usize>,
) -> anyhow::Result<(ProcessPromptEmbedOutput, ProcessPromptEmbedOutput)> {
    let padding = PaddingParams {
        strategy: PaddingStrategy::BatchLongest,
        direction: PaddingDirection::Left,
        pad_to_multiple_of: None,
        pad_id: eos_token_id,
        pad_type_id: 0,
        pad_token: String::from(eos_token),
    };

    tokenizer.with_padding(Some(padding));
    let encoded = tokenizer
        .encode_batch(prompts.to_vec(), true)
        .map_err(Error::msg)?;
    let tokens: Vec<Vec<u32>> = encoded.iter().map(|x| x.get_ids().to_vec()).collect();
    let mask: Vec<Vec<u32>> = encoded
        .iter()
        .map(|x| x.get_attention_mask().to_vec())
        .collect();

    // let tokens: Vec<u32> = encoded.iter().flat_map(|x| x.get_ids().into_iter().map(|x| *x)).collect();
    // let mask: Vec<u32> = encoded.iter().flat_map(|x| x.get_attention_mask().into_iter().map(|x| *x)).collect();
    // dbg!(&mask);

    // let (tokens, mask) = match max_seq_length {
    //     Some(msl) => {
    //         if tokens.len()  > msl - 10 {
    //             let to_remove = tokens.len() + 10 - msl;
    //             if to_remove > tokens.len() {
    //                 return Err(anyhow!("The prompt size is {}, the maximum allowable sequence length is {}, and the remove length is {} which is greater than the prompt size!",
    //                 tokens.len(), msl, to_remove));
    //             }
    //             (tokens[tokens.len().saturating_sub(to_remove)..].to_vec(),
    //             mask[mask.len().saturating_sub(to_remove)..].to_vec())
    //         } else {
    //             (tokens, mask)
    //         }
    //     },
    //     None => (tokens, mask)
    // };

    Ok((tokens, mask))
}
#[cfg(test)]
mod tests {
    use crate::candle_assets::candle_which::{load_model_asset_path, load_tokenizer};

    use super::*;

    #[test]
    fn creat_prompt_embed_test() {
        let query_template: &str = "{instructions}{query}<|endoftext|>}";

        let query_metadata = "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: ".to_string();
        let query_data = vec![
            "how much protein should a female eat".to_string(),
            "summit define".to_string(),
        ];

        if let Ok(query_strs) = create_prompt_embed(query_template, &query_data, &query_metadata) {
            let expect: Vec<String> = vec!["Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: how much protein should a female eat<|endoftext|>}".to_string(), 
                "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: summit define<|endoftext|>}".to_string()];
            assert_eq!(query_strs, expect);
        }

        let document_metadata = "".to_string();
        let document_data = vec!["As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.".to_string(), 
        "Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.".to_string()];

        if let Ok(query_strs) =
            create_prompt_embed(query_template, &document_data, &document_metadata)
        {
            let expect: Vec<String> = vec!["As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.<|endoftext|>}".to_string(),
                "Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.<|endoftext|>}".to_string()];
            assert_eq!(query_strs, expect);
        }
    }

    #[test]
    fn process_prompt_embed_test() {
        let prompts: Vec<String> = vec!["As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.<|endoftext|>}".to_string(),
        "Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.<|endoftext|>}".to_string()];
        let eos_token: &str = "<|endoftext|>";
        let eos_token_id: u32 = 151643;

        let path: Option<String> = Some(format!(
            "{}/.cache/hf/models--sentence-transformers--all-MiniLM-L6-v2/tokenizer.json",
            std::env::var("HOME").unwrap_or("".to_string())
        ));
        let repo = "sentence-transformers/all-MiniLM-L6-v2".to_string();
        let filename = "tokenizer.json".to_string();
        let revision = "main".to_string();
        let max_seq_length: Option<usize> = Some(2048);
        let mut tokenizer =
            load_tokenizer(load_model_asset_path(&path, &repo, &filename, &revision))
                .expect("Tokenizer failed to load!");

        if let Ok((tokens, masks)) = process_prompt_embed(
            &prompts,
            &mut tokenizer,
            eos_token_id,
            eos_token,
            max_seq_length,
        ) {
            let tokens_expected: Vec<u32> = vec![101, 2004, 1037];
            let masks_expected: Vec<u32> = vec![1, 1, 1];
            assert_eq!(&tokens[0][0..3], tokens_expected.as_slice());
            assert_eq!(&masks[0][0..3], masks_expected.as_slice());
        }
    }
}
